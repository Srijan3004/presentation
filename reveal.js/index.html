<!doctype html>
<html>
    <head>
        <meta charset="utf-8">

        <title>Resampling Techniques </title>

        <meta name="description" content="SP2020">
        <meta name="author" content="Srijan Chattopadhyay">

        <meta name="apple-mobile-web-app-capable" content="yes">
        <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

        <meta name="viewport" content="width=device-width, initial-scale=1.0">

        <link rel="stylesheet" href="dist/reset.css">
        <link rel="stylesheet" href="dist/reveal.css">
        <link rel="stylesheet" href="dist/theme/night.css" id="theme">

        <!-- Theme used for syntax highlighting of code -->
        <link rel="stylesheet" href="plugin/highlight/monokai.css" id="highlight-theme">
        <!-- <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css"> -->
        <link href="css/font-awesome-5.1.0/css/all.css" rel="stylesheet" />
        <link href="css/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet" />
    </head>
    <body>
        <div class="reveal">
            <div class="slides">
                <section data-background="network.gif" data-background-opacity=0.4 style="font-size: 33px;" data-auto-animate
				data-auto-animate-easing="cubic-bezier(0.770, 0.000, 0.175, 1.000)">
                    <h3 id="heading" style="color: white; text-align: center; margin-bottom: 0.8cm; font-size: 60px;">
						<font color="#FFFF00">  
							<b>Resampling Techniques</b> 
						</font> 
                       
                	</h3>

					
                    <hr>
                    <div style="text-align: center; margin-top: 1.5cm;">
                        Topic Presentation:
                        <br>
					</div>
					<div style="text-align: center; margin-top: 0.5cm;">
                        <strong>Srijan Chattopadhyay</strong>
                        <br>
                        B.Stat 3rd Year<br>
						Indian Statistical Institute, Kolkata
                    </div>
					<div style="text-align: center; margin-top: 0.8cm;">
                        BS2126
                        <br>
					</div>
                </section>

                	<section data-background="bg-2.jpeg" data-background-opacity=0.4 style="text-align: left; font-size: 26px;" data-auto-animate
				data-auto-animate-easing="cubic-bezier(0.770, 0.000, 0.175, 1.000)">
				<h3>
					<font color="#FFFF00">  
						Introduction <br>  
					</font> 
				</h3>
			    <section data-transition="fade-in slide-out">
                    <h3>
						Why it is needed? 
                    </h3>
				    <center>
					    \vec{X} = (X_1,X_2,\cdots,X_n) \hspace{0.2cm}\text{iid data}
				    </center>
					    
					
					<div>
						Say, we want to estimate something from the distribution of the test statistic based on the current data...But we don't have any extra data left to make any comment about any distributional feature of the test statistic of the data...Well, what can we do is like a splitting of the data and get multiple observations of the test statistic. But...for that we want the actual data to be too large! That's not always possible to have...So we have to use the same data repeatedly just assuming that as if the data is the whole population.
					</div>
                   
			
                </section>

				<section data-background="bg-2.jpeg" data-background-opacity=0.4 style="text-align: left; font-size: 26px;">
                    <h3>
                        What is cluster analysis?</h3>
					<div>
						
					</div>
					<div>

                    Clustering is an unsupervised method for partitioning a collection of data
                    points, into groups, such that the objects in a group will be similar (or related)
				    to one another and different from (or unrelated to) the objects in other groups.
					</div>
                    <center>
						<img src="clustering.gif" class="centerImage" height="320" width="500" >
					 </center>
			
                </section>

				<section data-background="bg-2.jpeg" data-background-opacity=0.4 style="text-align: left; font-size: 26px;">
                    <h4>
					Traditional Methods of Clustering
                    </h4>
                    <div>
						<ul>
							<li class="fragment">
								K-means clustering 
							</li>
						</ul>
					</div>

					<div style="margin-top: 0.6cm;">
						<ul>
							<li class="fragment">
								Hierarchical Clustering
							</li>
						</ul>
					</div>
					<div style="margin-top: 0.6cm;"><ul>
							<li class="fragment">
								Spectral Clustering 
							</li>
						</ul>
					</div>
					<div style="margin-top: 0.6cm;">
						<ul>
							<li class="fragment">
								Fuzzy Clustering 
							</li>
							
						</ul>
						
					</div>
					<div style="margin-top: 0.6cm;">
						And so on....
					</div>
					<hr>
					
                </section>

			<section data-background="bg-2.jpeg" data-background-opacity=0.4 style="text-align: left; font-size: 26px;" data-auto-animate
				data-auto-animate-easing="cubic-bezier(0.770, 0.000, 0.175, 1.000)">
				<h4>
					<font color="#FFFF00">  
						Background <br>  
					</font> 
				</h4>
			    <section data-transition="fade-in slide-out">
                    <h4>
                        K-means clustering algorithm
                    </h4>
                    
                    <div>
						<ol>
							<li class="fragment">
								Decide on a value for the number of clusters, $k$.
							</li>
						
							<li class="fragment">
								Initialize the $k$ cluster centers (randomly).
							</li>
						    <li class="fragment">
								Repeat until convergence-
							</li>
							<ul>
								<li class="fragment">
									Decide the class memberships of the $N$ objects by assigning them to the nearest cluster center.	
								</li>
							
								<li class="fragment">
									Re-estimate the $k$ cluster centers, using the mean of each cluster.	
								</li>
						
						</ul>
						</0l>
						<center>
						<img src="K-means.gif" class="centerImage" height="320" width="380">
					    </center>
					</div>
					
				
				    </section>

					<section data-background="bg-2.jpeg" data-background-opacity=0.4 style="text-align: left; font-size: 26px;" data-transition="fade-in slide-out">
						<h3>
						K-means objective function
						</h3>
						
						<div>
						Note that, given a set of data points $\boldsymbol{x}_{1}, \ldots, \boldsymbol{x}_{n}$, the k-means algorithm attempts to find clusters $\ell_{1}, \ldots, \ell_{k}$ 
						to minimize the following objective function:
						$$
						\min_{\left\{\ell_{c}\right\}_{c=1}^{k}} \sum_{c=1}^{k} \sum_{\boldsymbol{x} \in \ell_{c}}\|\boldsymbol{x}-\boldsymbol{\mu}_{c}\|_2^2
						\text { , where }  \boldsymbol{\mu}_{c}=\dfrac{1}{\left|\ell_{c}\right|} \sum_{\boldsymbol{x} \in \ell_{c}} \boldsymbol{x} .
						$$
						</div>
						
				
					</section>	
                

					<section data-background="bg-2.jpeg" data-background-opacity=0.4 style="text-align: left; font-size: 26px;" data-transition="fade-in slide-out">
						<h3>
							Remarks
						</h3>
	
					    <div  style="margin-top: 0.6cm;">
						
						<ul>
						<li class="fragment" >	
							K-means is straightforward
							to implement and works well for a variety of applications.
							But, a major drawback of this algorithm is that the value of k needs to be specified by the user, which is not known in prior.

						</li>
					    </ul>
					    </div>

					    <div style="margin-top: 0.6cm;">
					    <ul>
						<li class="fragment">	
						Whereas, Bayesian nonparametric models, viz. the Dirichlet process mixture, result in infinite mixture models which do not
						fix the number of clusters in the data upfront; these methods
						continue to gain popularity in the statistical learning community. But, Bayesian models require sampling algorithms
						or variational inference techniques which can be difficult to
						implement and are often not scalable.
					    </li>
					    </ul>
					</div>
					</section>

					<section data-background="bg-2.jpeg" data-background-opacity=0.4 style="text-align: left; font-size: 26px;" data-transition="fade-in slide-out">
						<h3>
							Dirichlet process mixture model
						</h3>
						
						<div class="fragment">
							A Bayesian extension to the mixture model arises by first placing a Dirichlet prior,
							$\operatorname{Dir}\left(k, \boldsymbol{\pi}_{0}\right)$, on the mixing coefficients,
							for some $\boldsymbol{\pi}_{0}$. If we further assume that the covariances of the Gaussians
							are fixed to $\sigma I$ and that the means are drawn from some prior distribution $G_{0}$,
						    we obtain the following Bayesian model:

						</div>
						<div class="fragment">
							$$
							\begin{aligned}
							\boldsymbol{\pi} & \sim \operatorname{Dir}\left(k, \boldsymbol{\pi}_{0}\right) \\
							\boldsymbol{z}_{1}, \ldots, \boldsymbol{z}_{n} & \sim \operatorname{Discrete}(\boldsymbol{\pi}) \\
							\boldsymbol{\mu}_{1}, \ldots, \boldsymbol{\mu}_{k} & \sim G_{0} \\
							\boldsymbol{x}_{1}, \ldots, \boldsymbol{x}_{n} & \sim \mathscr{N}\left(\boldsymbol{\mu}_{\boldsymbol{z}_{i}}, \sigma I\right)
							\end{aligned}
							$$
						</div>
						<div class="fragment">
							If we take $\boldsymbol{\pi}_{0}=(\alpha/k)\boldsymbol{1}$, the limit is the above model, as $k \to \infty$, is known as Dirichlet process mixture model.
						</div>
					
					</section>
				</section>

                <section data-background="bg-2.jpeg" data-background-opacity=0.4 style="text-align: left; font-size: 25px;" data-auto-animate
				data-auto-animate-easing="cubic-bezier(0.770, 0.000, 0.175, 1.000)">
				<h4>
					<font color="#FFFF00">  
						Hard Clustering via Dirichlet Process <br>  
					</font> 
				</h4>
				   <section data-transition="fade-in slide-out">
					<h4>
					Gibbs Sampling
					</h4>
					<div class="fragment">
						One of the simplest algorithm for inference in a DP mixture is based on Gibbs sampling.
						Gibbs sampling is a Markov chain Monte Carlo (MCMC) algorithm for obtaining a sequence of observations which are approximated from a specified multivariate probability distribution, when direct sampling is difficult.
						Gibbs sampling consists of a random walk on an undirected graph
						whose vertices, $V$ correspond to the values of $\boldsymbol{x} = (x_1, \cdots, x_d )$ and in which there is
						an edge from $\boldsymbol{x}$ to $\boldsymbol{y}$ if $\boldsymbol{x}$ and $\boldsymbol{y}$ differ in only one coordinate. 
						The steps of the algorithm are as follows:
					<div>
						<ul>
							<li class="fragment">
								Choose an initial assignment.
							</li>
						</ul>
					</div>

					<div style="margin-top: 0.6cm;">
						<ul>
							<li class="fragment">
								Fix an ordering of the variables (any order is fine).
							</li>
						</ul>
					</div>
					<div style="margin-top: 0.6cm;">
						<ul>
							<li class="fragment">
								For each $j \in V$ in order, draw a sample $z$ from $p(x_j|x_1^{t+1},\cdots,x_{j-1}^{t+1},x_{j+1}^{t},x_{n}^{t})$
							</li>
						
						</ul>
					</div>
					<div style="margin-top: 0.6cm;">
						<ul>
							<li class="fragment">
								Set $t \xleftarrow{} t + 1$ and repeat from step 2.
							</li>
						
						</ul>
					</div>
					
				    </section>
				    <section data-transition="fade-in slide-out">
						<h4>
						DP Gibbs Sampler 
						</h4>
						<div class="fragment">
						 The algorithm for inference in a DP mixture model based on Gibbs sampling is as follows:

						</div>

						<div>
							<ul>
								<li class="fragment">
									Let the state of the underlying Markov chain
									consists of the set of all cluster indicators and the set of
									all cluster means.
								</li>
							</ul>
						</div>
	
						<div style="margin-top: 0.6cm;">
							<ul>
								<li class="fragment">
									The algorithm proceeds by first looping
									repeatedly through each of the data points and performing
									Gibbs moves on the cluster indicators for each point.
								</li>
							</ul>
						</div>
						<div style="margin-top: 0.6cm;">
							<ul>
								<li class="fragment">
									For $i=1, \ldots, n$, we reassign $\boldsymbol{x}_{i}$ to existing cluster $c$ with probability $\frac{n_{-i, c}}{Z} \cdot \phi\left(\boldsymbol{x}_{i} \mid \boldsymbol{\mu}_{c}, \sigma I\right)$, where $n_{-i, c}$ is the number of data points (excluding $\boldsymbol{x}_{i}$) that are assigned to cluster $c$. With probability
									$
									\frac{\alpha}{Z} \int \phi\left(\boldsymbol{x}_{i} \mid \boldsymbol{\mu}, \sigma I\right) d G_{0}(\boldsymbol{\mu})
									$,
									we start a new cluster. $Z$ is an appropriate normalizing constant here.

                                    If we end up choosing to start a new cluster, we select its mean from the posterior distribution obtained from the prior $G_{0}$ and the single sample $\boldsymbol{x}_{i}$. 
								</li>
							
							</ul>
						</div>
						<div style="margin-top: 0.6cm;">
							<ul>
								<li class="fragment">
									Next, we perform Gibbs moves on the means: sample $\boldsymbol{\mu}_{c}$ from the posterior based on prior $G_0$ and all points currently assigned to cluster $c, \forall c$.
								</li>
							
							</ul>
						</div>
						
					</section>
					<section data-background="bg-2.jpeg" data-background-opacity=0.4 style="text-align: left; font-size: 26px;" data-transition="fade-in slide-out">
							<h4>
								DP Gibbs Sampler 
							</h4>
							
							<div>
								Then, we have the following probabilities to be
								used during Gibbs sampling:
							</div>

							<div style="margin-top: 0.6cm;">
								<ul>
									<li class="fragment">
										The posterior probability
										of point $i$ being assigned to cluster $c$,
										$$\hat{\gamma}\left(z_{i c}\right)=\frac{n_{-i, c} \cdot \exp \left(-\frac{1}{2 \sigma}\|\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\|^{2}\right)}{\exp \left(-\frac{\lambda}{2 \sigma}-\frac{\|\boldsymbol{x}_{i}\|^{2}}{2(\rho+\sigma)}\right)+\sum_{j=1}^{k} n_{-i, j} \cdot \exp \left(-\frac{1}{2 \sigma}\|\boldsymbol{x}_{i}-\boldsymbol{\mu}_{j}\|^{2}\right)}$$
									</li>
								
								</ul>
							</div>

							<div style="margin-top: 0.6cm;">
								<ul>
									<li class="fragment">
								     and the posterior probability of starting a new cluster
                                     $$\hat{\gamma}\left(z_{i, n e w}\right)=\frac{\exp \left(-\frac{\lambda}{2 \sigma}-\frac{\|\boldsymbol{x}_{i}\|^{2}}{2(\rho+\sigma)}\right)}{\exp \left(-\frac{\lambda}{2 \sigma}-\frac{\|\boldsymbol{x}_{i}\|^{2}}{2(\rho+\sigma)}\right)+\sum_{j=1}^{k} n_{-i, j} \cdot \exp \left(-\frac{1}{2 \sigma}\|\boldsymbol{x}_{i}-\boldsymbol{\mu}_{j}\|^{2}\right)}.$$ 
									</li>
								
								</ul>
							</div>
						
					</section>

					<section data-background="bg-2.jpeg" data-background-opacity=0.4 style="text-align: left; font-size: 26px;" data-transition="fade-in slide-out">
							<h4>
								Asymptotics of the DP Gibbs Sampler
							</h4>
							
							<div>
								<ul>
									<li class="fragment">
										As $\sigma \rightarrow 0$, the values of $\hat{\gamma}\left(z_{i, c}\right)$ and $\hat{\gamma}\left(z_{i, n e w}\right)$ will be increasingly dominated 
										by the smallest value of $\left\{\|\boldsymbol{x}_{i}-\boldsymbol{\mu}_{1}\|^{2}, \cdots, \| \boldsymbol{x}_{i}-\boldsymbol{\mu}_{k} \|^{2}, \lambda\right\}$ and only the smallest of these values will receive a non-zero $\hat{\gamma}$ value.
									</li>
								</ul>
							</div>
		
							<div style="margin-top: 0.6cm;">
								<ul>
									<li class="fragment">
										This implies reassignment a point to the cluster corresponding to the closest mean, 
										if the closest cluster has squared distance smaller than $\lambda$. Otherwise, 
										we start a new cluster, in which case we sample a new mean from the posterior based on the prior $G_{0}$ and
										single observation $\boldsymbol{x}_{i}$
									</li>
								</ul>
							</div>
							<div style="margin-top: 0.6cm;">
								<ul>
									<li class="fragment">
										Similarly, once we have performed Gibbs moves on the cluster assignments, we must perform Gibbs moves on all the means, which amounts to sampling from the posterior based on $G_{0}$ and all observations in a cluster.
									</li>
								
								</ul>
							</div>
							
						
					</section>

					<section data-background="bg-2.jpeg" data-background-opacity=0.4 style="text-align: left; font-size: 26px;" data-transition="fade-in slide-out">
							<h4>
								DP Gibbs Sampler 
							</h4>
							
							<div>
								Then, we have the following probabilities to be
								used during Gibbs sampling:
							</div>

							<div style="margin-top: 0.6cm;">
								<ul>
									<li class="fragment">
										The posterior probability
										of point $i$ being assigned to cluster $c$,
										$$\hat{\gamma}\left(z_{i c}\right)=\frac{n_{-i, c} \cdot \exp \left(-\frac{1}{2 \sigma}\|\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\|^{2}\right)}{\exp \left(-\frac{\lambda}{2 \sigma}-\frac{\|\boldsymbol{x}_{i}\|^{2}}{2(\rho+\sigma)}\right)+\sum_{j=1}^{k} n_{-i, j} \cdot \exp \left(-\frac{1}{2 \sigma}\|\boldsymbol{x}_{i}-\boldsymbol{\mu}_{j}\|^{2}\right)}$$
									</li>
								
								</ul>
							</div>

							<div style="margin-top: 0.6cm;">
								<ul>
									<li class="fragment">
								     and the posterior probability of starting a new cluster
                                     $$\hat{\gamma}\left(z_{i, n e w}\right)=\frac{\exp \left(-\frac{\lambda}{2 \sigma}-\frac{\|\boldsymbol{x}_{i}\|^{2}}{2(\rho+\sigma)}\right)}{\exp \left(-\frac{\lambda}{2 \sigma}-\frac{\|\boldsymbol{x}_{i}\|^{2}}{2(\rho+\sigma)}\right)+\sum_{j=1}^{k} n_{-i, j} \cdot \exp \left(-\frac{1}{2 \sigma}\|\boldsymbol{x}_{i}-\boldsymbol{\mu}_{j}\|^{2}\right)}.$$ 
									</li>
								
								</ul>
							</div>
						
					</section>

					<section data-background="bg-2.jpeg" data-background-opacity=0.4 style="text-align: left; font-size: 26px;" data-transition="fade-in slide-out">
							<h4>
								Asymptotics of the DP Gibbs Sampler
							</h4>
							
							<div>
								<ul>
									<li class="fragment">
										As $\sigma \rightarrow 0$, the values of $\hat{\gamma}\left(z_{i, c}\right)$ and $\hat{\gamma}\left(z_{i, n e w}\right)$ will be increasingly dominated 
										by the smallest value of $\left\{\|\boldsymbol{x}_{i}-\boldsymbol{\mu}_{1}\|^{2}, \cdots, \| \boldsymbol{x}_{i}-\boldsymbol{\mu}_{k} \|^{2}, \lambda\right\}$ and only the smallest of these values will receive a non-zero $\hat{\gamma}$ value.
									</li>
								</ul>
							</div>
		
							<div style="margin-top: 0.6cm;">
								<ul>
									<li class="fragment">
										This implies reassignment a point to the cluster corresponding to the closest mean, if the closest cluster has squared distance smaller than $\lambda$. Otherwise, we start a new cluster, in which case we sample a new mean from the posterior based on the prior $G_{0}$ and  single observation 
									</li>
								</ul>
							</div>
							<div style="margin-top: 0.6cm;">
								<ul>
									<li class="fragment">
										Similarly, once we have performed Gibbs moves on the cluster assignments, we must perform Gibbs moves on all the means, which amounts to sampling from the posterior based on $G_{0}$ and all observations in a cluster.
									</li>
								
								</ul>
							</div>
							
						
					</section>
                    <section data-background="bg-2.jpeg" data-background-opacity=0.4 style="text-align: left; font-size: 26px;" data-transition="fade-in slide-out">
							
						<div style="margin-top: 0.6cm;">
							<ul>
								<li class="fragment">
									Since the prior and likelihood are Gaussian, the posterior will be Gaussian as well. If we let $\overline{\boldsymbol{x}}_{c}$ be the mean of the points currently assigned to cluster $c$ and $n_{c}$ be the number of points assigned to cluster $c$, then the
									posterior is a Gaussian with mean $\tilde{\boldsymbol{\mu}}_{c}$ and covariance $\tilde{\Sigma}_{c}$, where
									$
									\tilde{\boldsymbol{\mu}}_{c}=\left(1+\frac{\sigma}{\rho n_{c}}\right)^{-1} \overline{\boldsymbol{x}}_{c}, \quad \tilde{\Sigma}_{c}=\frac{\sigma \rho}{\sigma+\rho n_{c}} I
									$.
								</li>
							
							</ul>
						</div>
						<div style="margin-top: 0.6cm;">
							<ul>
								<li class="fragment">
									As $\sigma \rightarrow 0$, $\tilde{\boldsymbol{\mu}}_{c}\to \overline{\boldsymbol{x}}_{c}$ and $\tilde{\Sigma}_{c}\to \mathbf{0}$,
									meaning that the mass of the distribution becomes concentrated at $\overline{\boldsymbol{x}}_{c}$. Thus, in the limit we choose $\overline{\boldsymbol{x}}_{c}$ as the mean,
									which leads us to:
								</li>
							
							</ul>
						</div>
						
						</section>
		
					</section>

				<section data-background="bg-2.jpeg" data-background-opacity=0.4 style="text-align: left; font-size: 26px;" data-auto-animate
						data-auto-animate-easing="cubic-bezier(0.770, 0.000, 0.175, 1.000)">
				
					<section data-transition="fade-in slide-out">
						<h3>
							<font color="#FFFF00">  
								Algorithm 1 : DP-means <br>  
								</font> 
							
						</h3>
						
						<div class="fragment">
							<font color="#FFFF01">  
								Input :
							</font>
							$\boldsymbol{x}_1, \cdots, \boldsymbol{x}_n$ : input data, $\lambda:$ cluster penalty parameter
						</div>
						<div class="fragment">
							<font color="#FFFF01">  
								Output:
							</font>
							Clustering $\ell_1, \cdots, \ell_k$ and number of clusters $k$
						</div>
						<div class="fragment">
							<ul>
								<li class="fragment">
									<font color="#FFFF01">  
										Step 1.
									</font> 
									Initialize: $k=1, \ell_1=\left\{\boldsymbol{x}_1, \cdots, \boldsymbol{x}_n\right\}$ and $\boldsymbol{\mu}_1$ the global mean.
								</li>
								<li class="fragment">
									<font color="#FFFF01">  
										Step 2.
									</font> 
									Initialize: cluster indicators $z_i=1$ for all $i=1, \ldots, n$.
								</li>
								<li class="fragment">
									<font color="#FFFF01">  
										Step 3.
									</font> 
									Repeat until convergence:
									<ol>
										<li>
											For each point $\boldsymbol{x}_i$, compute $d_{i c}=\|\boldsymbol{x}_i-\boldsymbol{\mu}_c\|^2$ for $c=1, \ldots, k$
										</li>

										<li>
											If $\min _c d_{i c}>\lambda$, set $k=k+1, z_i=k$, and $\boldsymbol{\mu}_k=\boldsymbol{x}_i$.
										 Otherwise, set $z_i=\operatorname{argmin}_c d_{i c}$.

										</li>
										<li>
											Generate clusters $\ell_1, \ldots, \ell_k$ based on $z_1, \ldots, z_k: \ell_j=$ $\left\{\boldsymbol{x}_i \mid z_i=j\right\}$
										</li>
										<li>
											For each cluster $\ell_j$, compute $\boldsymbol{\mu}_j=\frac{1}{\left|\ell_j\right|} \sum_{\boldsymbol{x} \in \ell_j} \boldsymbol{x}$.
										</li>
									</ol>

								</li>
							</div>
							
					</section>

					<section data-transition="fade-in slide-out">

						<h3>
							<font color="#FFFF00">  
								Convergence of Objective function of DP means <br>  
							</font> 
							
						</h3>
							<div class="fragment">
							
								<font color="#FFFF00">  
									Theorem: 
								</font>  DP means algorithm  monotonically decreases the following objective until local convergence:
								$$
								\begin{array}{cc}
								\min _{\left\{\ell_{c}\right\}_{c=1}^{k}} & \sum_{c=1}^{k} \sum_{\boldsymbol{x} \in \ell_{c}}\|\boldsymbol{x}-\boldsymbol{\mu}_{c}\|^{2}+\lambda k \\
								\text { where } & \boldsymbol{\mu}_{c}=\frac{1}{\left|\ell_{c}\right|} \sum_{\boldsymbol{x} \in \ell_{c}} \boldsymbol{x} .
								\end{array}
								$$	
							
							</div>
							<div class="fragment">
							
								<font color="#FFFF00">  
									Proof:
								</font>
									The reassignment step in the algorithm results in a non-increasing objective since the distance between
									 a point and its newly assigned cluster mean never increases; for distances greater than $\lambda$,
									  we can generate a new cluster and pay a penalty of $\lambda$ while still decreasing the objective.
									 Similarly, the mean update step results in a non-increasing objective since the mean is the best representative of a cluster in terms of the squared Euclidean distance.
									  The fact that the algorithm will converge locally follows from the fact that the objective function cannot increase,
									and that there are only a finite number of possible clusterings of the data.
							</div>
					</section>	
				</section>
 

				<section data-background="bg-2.jpeg" data-background-opacity=0.4 style="text-align: left; font-size: 26px;" data-auto-animate
				data-auto-animate-easing="cubic-bezier(0.770, 0.000, 0.175, 1.000)">
				<h3>
					<font color="#FFFF00">  
						Experimental Results <br>  
					</font> 
				</h3>
				<section data-transition="fade-in slide-out">
				<h4>
					Normalized Mutual Information (NMI)
				</h4>
				<div>
					Normalized Mutual Information between class labels $Y$ and cluster labels $C$ is defines as:
					$$NMI(Y,C)=\frac{2\times I(Y,C)}{H(Y)+H(C)}$$
				    where, $H$ is the entropy, $I(Y,C)$ is the mutual information between $Y$ and $C$. 
					Mutual Information tells us the reduction in the entropy of class labels that we get if we know the cluster labels.
					NMI takes value between 0 and 1.
				</div>
				<div style="margin-top: 0.6cm;">
					<ul>
						<li class="fragment">
							NMI is a standard measure for determining the quality of clustering	. Higher the NMI, better the quality of clustering.
						</li>
						<li class="fragment">
							Since it is normalized we can measure and
							compare the NMI between different
							clusterings having different number of
							clusters.
						</li>
					
					</ul>
				</div>

				</section>
				<section data-transition="fade-in slide-out">
					<div>
					To demonstrate advantage of this method over DP Gibbes sampler, a synthetic data set of three Gaussians, shown in the left figure was used.
					The middle figure shows the NMI scores of the clusterings produced by DP means and two Gibbs runs (one that learns  $\alpha$ via a
					gamma prior, and another that uses a validation set to tune $\alpha$.)over the first 5000 iterations.
					Additionally, the number of clusters produced by DP-means as a function of $\lambda$ is plotted in right figure, where we see that there is a large interval of $\lambda$
					values where the algorithm returns three clusters.
				    </div>
					<img width="300" src="data.png">
                    <img width="300" src="tune.png">
					<img width="310" src="nmi.png">
				
					</section>
				    <section data-transition="fade-in slide-out">
					<div class="fragment">
						To demonstrate
						comparable accuracies among the methods- k-means, DP-means, and Gibbs sampling, 8
						common UCI data sets were selected.
						in making comparisons to k-means we fix $k$.
						Given an (approximate)
						number of desired clusters $k$, we first initialize a set
						$T$ with the global mean. We iteratively add to $T$ by finding
						the point in the data set which has the maximum distance to
						$T$ (the distance to $T$ is the smallest distance among points
						in $T$). We repeat this $k$ times and return the value of the
						maximum distance to $T$ in round $k$ as $\lambda$.
						The NMI (normalized
						mutual information) is computed between the ground-truth and the computed clusters, and results are averaged over 10 runs. The
						results are shown in the following table:
					
					</div>
				</section>
				<section data-transition="fade-in slide-out">
					<div >
						Table 1. Average NMI scores on a set of UCI data sets.
						<table>
							<tr>
								<td><b>Dataset</b></td>
								<td><b>DP Means</b></td>
								<td><b>K-means</b></td>
								<td><b>Gibbs</b></td>
							</tr>
							<tr>
								<td>Soybean</td>
								<td><font color="#00FF00">  
									.72
								</font></td>
								<td>.66</td>
								<td>.72</td>
							</tr>
							<tr>
								<td>Wine</td>
								<td>.41</td>
								<td>.43</td>
								<td>.42</td>
							</tr>
							<tr>
								<td>Breast Cancer</td>
								<td><font color="#00FF00">  
									.04
								</font></td>
								<td>.03</td>
								<td>.04</td>
							</tr>
							<tr>
								<td>Car</td>
								<td>.07</td>
								<td>.05</td>
								<td>.15</td>
							</tr>
							<tr>
								<td>Balance Scale</td>
								<td><font color="#00FF00">  
									.17
								</font></td>
								<td>.11</td>
								<td>.14</td>
							</tr>
							<tr>
								<td>Vehicle</td>
								<td><font color="#00FF00">  
									.18
								</font></td>
								<td>.18</td>
								<td>.17</td>
							</tr>
							<tr>
								<td>Iris</td>
								<td>.75</td>
								<td>.76</td>
								<td>.73</td>
							</tr>
							<tr>
								<td>Pima</td>
								<td>.02</td>
								<td>.03</td>
								<td>.02</td>
							</tr>
							<hr>
						</table>
					</div>
				    </section>
				</section>
				<section data-background="bg-2.jpeg" data-background-opacity=0.4 style="text-align: left; font-size: 26px;" data-transition="fade-in slide-out">
                    <h3>
						<font color="#FFFF00">  
						Conclusion <br>  
						</font> 
					</h3>
				    <div>
                    This paper outlines
					connections arising between DP mixture models and hard
					clustering algorithms, and develops scalable algorithms, like k-means for
					hard clustering that is capable of retaining some of the benefits of Bayesian
					nonparametric. This algorithm do not require to specify the number of clusters,
					which is a major advantage over k-means. But, it has some disadvantages also, for example,
					the method involves a hyperparameter and sometimes it is difficult to tune the hyperparameter.
				    </div>
					
				</section>
				
				<section data-background="source.gif" data-background-opacity=0.7 style="font-size: 33px;" >
                    <h3 style="color: rgb(11, 202, 245); text-align: center; margin-bottom: 0.8cm; font-size: 60px;">
                        <strong>Thank you</strong>
                    </h3>
				</section>
				
            </div>
        </div>
        <script src="dist/reveal.js"></script>
        <script src="plugin/notes/notes.js"></script>
        <script src="plugin/markdown/markdown.js"></script>
        <script src="plugin/zoom/zoom.js"></script>
        <script src="plugin/highlight/highlight.js"></script>
        <script src="plugin/math/math.js"></script>
        <script src="plugin/chalkboard/plugin.js"></script>
        <script>
            // More info about initialization & config:
            // - https://revealjs.com/initialization/
            // - https://revealjs.com/config/
            Reveal.initialize({
                hash: true,
                transition: 'concave',
				backgroundTransition: 'slide',
                // controls: false,
				controls: true,

                // Display a presentation progress bar
                progress: true,

                // Push each slide change to the browser history
                history: false,

                // Enable keyboard shortcuts for navigation
                keyboard: true,

				slideNumber: true,

				// Can be used to limit the contexts in which the slide number appears
				// - "all":      Always show the slide number
				// - "print":    Only when printing to PDF
				// - "speaker":  Only in the speaker view
				showSlideNumber: 'all',				

                // Loop the presentation
                loop: false,

                // Number of milliseconds between automatically proceeding to the 
                // next slide, disabled when set to 0
                autoSlide: 0,

                // Enable slide navigation via mouse wheel
                mouseWheel: false,

                // Apply a 3D roll to links on hover
                rollingLinks: true,
                // Learn about plugins: https://revealjs.com/plugins/
                // chalkboard: {
                    // src: "chalkboard/chalkboard.json",
                    // toggleChalkboardButton: false,
                    // toggleNotesButton: false,
                // },
                plugins: [ RevealMarkdown, RevealHighlight, RevealNotes, RevealMath ],
            });
        </script>
    </body>
</html>
