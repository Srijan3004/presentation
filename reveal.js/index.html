<!doctype html>
<html>
    <head>
        <meta charset="utf-8">

        <title>Resampling Techniques </title>

        <meta name="description" content="SP2020">
        <meta name="author" content="Srijan Chattopadhyay">

        <meta name="apple-mobile-web-app-capable" content="yes">
        <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

        <meta name="viewport" content="width=device-width, initial-scale=1.0">

        <link rel="stylesheet" href="dist/reset.css">
        <link rel="stylesheet" href="dist/reveal.css">
        <link rel="stylesheet" href="dist/theme/night.css" id="theme">

        <!-- Theme used for syntax highlighting of code -->
        <link rel="stylesheet" href="plugin/highlight/monokai.css" id="highlight-theme">
        <!-- <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css"> -->
        <link href="css/font-awesome-5.1.0/css/all.css" rel="stylesheet" />
        <link href="css/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet" />
    </head>
    <body>
        <div class="reveal">
            <div class="slides">
                <section data-background="network.gif" data-background-opacity=0.4 style="font-size: 33px;" data-auto-animate
				data-auto-animate-easing="cubic-bezier(0.770, 0.000, 0.175, 1.000)">
                    <h3 id="heading" style="color: white; text-align: center; margin-bottom: 0.8cm; font-size: 60px;">
						<font color="#FFFF00">  
							<b>Resampling Techniques</b> 
						</font> 
                       
                	</h3>

					
                    <hr>
                    <div style="text-align: center; margin-top: 1.5cm;">
                        Topic Presentation:
                        <br>
					</div>
					<div style="text-align: center; margin-top: 0.5cm;">
                        <strong>Srijan Chattopadhyay</strong>
                        <br>
                        B.Stat 3rd Year<br>
						Indian Statistical Institute, Kolkata
                    </div>
					<div style="text-align: center; margin-top: 0.8cm;">
                        BS2126
                        <br>
					</div>
                </section>

                	<section data-background="bg-2.jpeg" data-background-opacity=0.4 style="text-align: left; font-size: 26px;" data-auto-animate
				data-auto-animate-easing="cubic-bezier(0.770, 0.000, 0.175, 1.000)">
				<h3>
					<font color="#FFFF00">  
						Introduction <br>  
					</font> 
				</h3>
			    <section data-transition="fade-in slide-out">
                    <h3>
						Why it is needed? 
                    </h3>
				    <center>
					    $\vec{X} = (X_1,X_2,\cdots,X_n) \hspace{0.2cm}\text{iid data}$
				    </center>
					    
					
					<div>
						Say, we want to estimate something from the 
						distribution of the test statistic based on the 
						current data...But we don't have any extra data 
						left to make any comment about any distributional 
						feature of the test statistic of the data...Well, 
						what can we do is like a splitting of the data and 
						get multiple observations of the test statistic. 
						But...for that we want the actual data to be too large! 
						That's not always possible to have...So we have to use 
						the same data repeatedly just assuming that as if the 
						data is the whole population.
					</div>
                   
			
                </section>

				<section data-background="bg-2.jpeg" data-background-opacity=0.4 style="text-align: left; font-size: 26px;">
                    <h3>
                        Usefulness</h3>
					<div>
						
					</div>
					<div>
						Say, we want to estimate some parameter of the actual population from the 
						observed data. $\theta$ is the actual parameter. Say, we estimate it by $\hat{\theta} = S_n(\vec{x})$.
						But, then the question is 
						how good and how stable the estimate is. i.e., 
						based on this data how far is the estimate from the actual. 
						Also, if this is not too far based on this data, then is that due 
						to the goodness of the estimate, i.e. if I take data from the same distribution 
						and redo the same procedure, am I going to get something close to it or too far? 
						For that, we have to get an idea about the bias and variance of the data. 
						But with only one sample in hand, how on earth are we going to do that?

                                                   <center>Thanks to Resampling!</center>
					</div>
                
			
                </section>
                </section>

			<section data-background="bg-2.jpeg" data-background-opacity=0.4 style="text-align: left; font-size: 36px;" data-auto-animate
				data-auto-animate-easing="cubic-bezier(0.770, 0.000, 0.175, 1.000)">
				<h4>
					<font color="#FFFF00">  
						Jackknife <br>  
					</font> 
				</h4>
			    <section data-transition="fade-in slide-out">
                    <h4>
                      What is it?  
                    </h4>
                    
                    <div>
			Jackknife is the most naive approach, just removing some k data points systematically 
			    to get essentially many new samples!		
		    </div>
					
				
		 </section>

		             <section data-background="bg-2.jpeg" data-background-opacity=0.4 style="text-align: left; font-size: 26px;" data-transition="fade-in slide-out">
						<h3>
						Method
						</h3>
						
						<div>
						Following is the -1 jackknife, i.e. removing 1 data point at a time..
							\begin{equation*}
							\vec{X}_{[i]} = (X_1,\cdots,X_{i-1},X_{i+1},\cdots,X_n)
							\end{equation*}
						
						We calculate the statistic from each of these replicates 
							\begin{equation*}
							\hat{\theta}_{[i]} = S_{n-1}(\vec{X}_{[i]}), \hspace{0.2cm} \text{where} \hspace{0.2cm} \hat{\theta} = S_n(\vec{X})
							\end{equation*}

							\begin{equation*}
							\hat{\theta}() = \frac{1}{n}\sum_{1}^{n}{\hat{\theta}_{[i]}}
							\end{equation*}
							
						</div>
						
				
	                      </section>	

				<section data-background="bg-2.jpeg" data-background-opacity=0.4 style="text-align: left; font-size: 26px;" data-transition="fade-in slide-out">
						<h3>
						Estimates of Bias and Variance
						</h3>
						
						<div>
						Jackknife estimate of bias is:
							\begin{equation*}
							\hat {Bias}_{Jack}(\hat{\theta}) = (n-1) (\hat{\theta}() - \hat{\theta})
							\end{equation*}
						
						And, Jackknife estimate of Variance is:
							\begin{equation*}
							\hat{V}_{Jack}(\hat{\theta}) = \frac{n-1}{n}\sum_{i=1}^{n}(\hat{\theta}_{[i]} - \hat{\theta}())^2
							\end{equation*}
							
						</div>
						
				
	                       </section>	

				<section data-background="bg-2.jpeg" data-background-opacity=0.4 style="text-align: left; font-size: 26px;" data-transition="fade-in slide-out">
						<h3>
						Why $\frac{n-1}{n}$?
						</h3>
						
						<div>
						The (n-1)/n factor is derived considering the special case $\hat{\theta} = \bar{X}$.
							\begin{equation*}
							\hat{\sigma}_{Jack} = \sqrt{\frac{1}{(n-1)n}\sum_{i=1}^{n}{(x_i - \bar{x})^2}}
							\end{equation*}
							is an unbiased estimator of the standard error of $\bar{X}$

						</div>
						
				
	                           </section>
				</section>

                <section data-background="bg-2.jpeg" data-background-opacity=0.4 style="text-align: left; font-size: 36px;" data-auto-animate
				data-auto-animate-easing="cubic-bezier(0.770, 0.000, 0.175, 1.000)">
				<h4>
					<font color="#FFFF00">  
						Bootstrap <br>  
					</font> 
				</h4>
				   <section data-transition="fade-in slide-out">
					<h4>
					Method
					</h4>
					<div class="fragment">
						
					<div>
						<ul>
							<li class="fragment">
								Take samples of the length same as the observed sample by SRSWR from the observed sample.
							</li>
						</ul>
					</div>

					<div style="margin-top: 0.6cm;">
						<ul>
							<li class="fragment">
								Take B many samples and for each of those, calculate the test statistic.
							</li>
						</ul>
					</div>
					<div style="margin-top: 0.6cm;">
						<ul>
							<li class="fragment">
								So, we have B many observations from the distribution of the test statistic.
							</li>
						
						</ul>
					</div>
					
				    </section>
				    <section data-transition="fade-in slide-out">
						<h4>
						Method
						</h4>

						<div>
							\begin{equation*}
							x_i ^ * \in \{x_1,\cdots,x_n\} \hspace{0.2cm} \text{for} \hspace{0.2cm} i = 1(1)n
							\end{equation*}
							\begin{equation*}
							\hat{\theta}^* = s(x_i ^ *)
							\end{equation*}
							\begin{equation*}
							\hat{\theta_1}^*, \hat{\theta_2}^*, \cdots, \hat{\theta_B}^*
							\end{equation*}
							
							So, using these B values, we can estimate many things about the estimated value of the actual parameter based on the observed data.
						                \begin{equation*}
								\hat{\theta^*} = \frac{1}{B}\sum_{i=1}^B{\hat{\theta_i}^*}
								\end{equation*}
							
						</div>
					    
					</section>
					<section data-background="bg-2.jpeg" data-background-opacity=0.4 style="text-align: left; font-size: 26px;" data-transition="fade-in slide-out">
							<h4>
								Estimates of Bias and Variance
							</h4>
							<div>
						Bootstrap estimate of bias is:
							\begin{equation*}
							\hat{Bias}_{Boot} = \hat{\theta^*} - \hat{\theta}
							\end{equation*}
						
						And, Bootstrap estimate of Variance is:
							\begin{equation*}
							\hat{V}_{Jack}(\hat{\theta}) = \frac{1}{B}\sum_{i=1}^{B}(\hat{\theta_i}^* - \hat{\theta^*}^2)
							\end{equation*}

								
                                                 \begin{equation*}
					         \frac{\hat{V}_{Jack}(\hat{\theta})}{V(\hat{\theta})} \overset{P}{\to} 1 \hspace{0.2cm} \text{Under Regularity Conditions}
					         \end{equation*}
						
							
						</div>
							
							
						
					</section>

					<section data-background="bg-2.jpeg" data-background-opacity=0.4 style="text-align: left; font-size: 26px;" data-transition="fade-in slide-out">
							<h4>
								Asymptotics of the DP Gibbs Sampler
							</h4>
							
							<div>
								<ul>
									<li class="fragment">
										As $\sigma \rightarrow 0$, the values of $\hat{\gamma}\left(z_{i, c}\right)$ and $\hat{\gamma}\left(z_{i, n e w}\right)$ will be increasingly dominated 
										by the smallest value of $\left\{\|\boldsymbol{x}_{i}-\boldsymbol{\mu}_{1}\|^{2}, \cdots, \| \boldsymbol{x}_{i}-\boldsymbol{\mu}_{k} \|^{2}, \lambda\right\}$ and only the smallest of these values will receive a non-zero $\hat{\gamma}$ value.
									</li>
								</ul>
							</div>
		
							<div style="margin-top: 0.6cm;">
								<ul>
									<li class="fragment">
										This implies reassignment a point to the cluster corresponding to the closest mean, 
										if the closest cluster has squared distance smaller than $\lambda$. Otherwise, 
										we start a new cluster, in which case we sample a new mean from the posterior based on the prior $G_{0}$ and
										single observation $\boldsymbol{x}_{i}$
									</li>
								</ul>
							</div>
							<div style="margin-top: 0.6cm;">
								<ul>
									<li class="fragment">
										Similarly, once we have performed Gibbs moves on the cluster assignments, we must perform Gibbs moves on all the means, which amounts to sampling from the posterior based on $G_{0}$ and all observations in a cluster.
									</li>
								
								</ul>
							</div>
							
						
					</section>

					<section data-background="bg-2.jpeg" data-background-opacity=0.4 style="text-align: left; font-size: 26px;" data-transition="fade-in slide-out">
							<h4>
								DP Gibbs Sampler 
							</h4>
							
							<div>
								Then, we have the following probabilities to be
								used during Gibbs sampling:
							</div>

							<div style="margin-top: 0.6cm;">
								<ul>
									<li class="fragment">
										The posterior probability
										of point $i$ being assigned to cluster $c$,
										$$\hat{\gamma}\left(z_{i c}\right)=\frac{n_{-i, c} \cdot \exp \left(-\frac{1}{2 \sigma}\|\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\|^{2}\right)}{\exp \left(-\frac{\lambda}{2 \sigma}-\frac{\|\boldsymbol{x}_{i}\|^{2}}{2(\rho+\sigma)}\right)+\sum_{j=1}^{k} n_{-i, j} \cdot \exp \left(-\frac{1}{2 \sigma}\|\boldsymbol{x}_{i}-\boldsymbol{\mu}_{j}\|^{2}\right)}$$
									</li>
								
								</ul>
							</div>

							<div style="margin-top: 0.6cm;">
								<ul>
									<li class="fragment">
								     and the posterior probability of starting a new cluster
                                     $$\hat{\gamma}\left(z_{i, n e w}\right)=\frac{\exp \left(-\frac{\lambda}{2 \sigma}-\frac{\|\boldsymbol{x}_{i}\|^{2}}{2(\rho+\sigma)}\right)}{\exp \left(-\frac{\lambda}{2 \sigma}-\frac{\|\boldsymbol{x}_{i}\|^{2}}{2(\rho+\sigma)}\right)+\sum_{j=1}^{k} n_{-i, j} \cdot \exp \left(-\frac{1}{2 \sigma}\|\boldsymbol{x}_{i}-\boldsymbol{\mu}_{j}\|^{2}\right)}.$$ 
									</li>
								
								</ul>
							</div>
						
					</section>

					<section data-background="bg-2.jpeg" data-background-opacity=0.4 style="text-align: left; font-size: 26px;" data-transition="fade-in slide-out">
							<h4>
								Asymptotics of the DP Gibbs Sampler
							</h4>
							
							<div>
								<ul>
									<li class="fragment">
										As $\sigma \rightarrow 0$, the values of $\hat{\gamma}\left(z_{i, c}\right)$ and $\hat{\gamma}\left(z_{i, n e w}\right)$ will be increasingly dominated 
										by the smallest value of $\left\{\|\boldsymbol{x}_{i}-\boldsymbol{\mu}_{1}\|^{2}, \cdots, \| \boldsymbol{x}_{i}-\boldsymbol{\mu}_{k} \|^{2}, \lambda\right\}$ and only the smallest of these values will receive a non-zero $\hat{\gamma}$ value.
									</li>
								</ul>
							</div>
		
							<div style="margin-top: 0.6cm;">
								<ul>
									<li class="fragment">
										This implies reassignment a point to the cluster corresponding to the closest mean, if the closest cluster has squared distance smaller than $\lambda$. Otherwise, we start a new cluster, in which case we sample a new mean from the posterior based on the prior $G_{0}$ and  single observation 
									</li>
								</ul>
							</div>
							<div style="margin-top: 0.6cm;">
								<ul>
									<li class="fragment">
										Similarly, once we have performed Gibbs moves on the cluster assignments, we must perform Gibbs moves on all the means, which amounts to sampling from the posterior based on $G_{0}$ and all observations in a cluster.
									</li>
								
								</ul>
							</div>
							
						
					</section>
                    <section data-background="bg-2.jpeg" data-background-opacity=0.4 style="text-align: left; font-size: 26px;" data-transition="fade-in slide-out">
							
						<div style="margin-top: 0.6cm;">
							<ul>
								<li class="fragment">
									Since the prior and likelihood are Gaussian, the posterior will be Gaussian as well. If we let $\overline{\boldsymbol{x}}_{c}$ be the mean of the points currently assigned to cluster $c$ and $n_{c}$ be the number of points assigned to cluster $c$, then the
									posterior is a Gaussian with mean $\tilde{\boldsymbol{\mu}}_{c}$ and covariance $\tilde{\Sigma}_{c}$, where
									$
									\tilde{\boldsymbol{\mu}}_{c}=\left(1+\frac{\sigma}{\rho n_{c}}\right)^{-1} \overline{\boldsymbol{x}}_{c}, \quad \tilde{\Sigma}_{c}=\frac{\sigma \rho}{\sigma+\rho n_{c}} I
									$.
								</li>
							
							</ul>
						</div>
						<div style="margin-top: 0.6cm;">
							<ul>
								<li class="fragment">
									As $\sigma \rightarrow 0$, $\tilde{\boldsymbol{\mu}}_{c}\to \overline{\boldsymbol{x}}_{c}$ and $\tilde{\Sigma}_{c}\to \mathbf{0}$,
									meaning that the mass of the distribution becomes concentrated at $\overline{\boldsymbol{x}}_{c}$. Thus, in the limit we choose $\overline{\boldsymbol{x}}_{c}$ as the mean,
									which leads us to:
								</li>
							
							</ul>
						</div>
						
						</section>
		
					</section>

				<section data-background="bg-2.jpeg" data-background-opacity=0.4 style="text-align: left; font-size: 26px;" data-auto-animate
						data-auto-animate-easing="cubic-bezier(0.770, 0.000, 0.175, 1.000)">
				
					<section data-transition="fade-in slide-out">
						<h3>
							<font color="#FFFF00">  
								Algorithm 1 : DP-means <br>  
								</font> 
							
						</h3>
						
						<div class="fragment">
							<font color="#FFFF01">  
								Input :
							</font>
							$\boldsymbol{x}_1, \cdots, \boldsymbol{x}_n$ : input data, $\lambda:$ cluster penalty parameter
						</div>
						<div class="fragment">
							<font color="#FFFF01">  
								Output:
							</font>
							Clustering $\ell_1, \cdots, \ell_k$ and number of clusters $k$
						</div>
						<div class="fragment">
							<ul>
								<li class="fragment">
									<font color="#FFFF01">  
										Step 1.
									</font> 
									Initialize: $k=1, \ell_1=\left\{\boldsymbol{x}_1, \cdots, \boldsymbol{x}_n\right\}$ and $\boldsymbol{\mu}_1$ the global mean.
								</li>
								<li class="fragment">
									<font color="#FFFF01">  
										Step 2.
									</font> 
									Initialize: cluster indicators $z_i=1$ for all $i=1, \ldots, n$.
								</li>
								<li class="fragment">
									<font color="#FFFF01">  
										Step 3.
									</font> 
									Repeat until convergence:
									<ol>
										<li>
											For each point $\boldsymbol{x}_i$, compute $d_{i c}=\|\boldsymbol{x}_i-\boldsymbol{\mu}_c\|^2$ for $c=1, \ldots, k$
										</li>

										<li>
											If $\min _c d_{i c}>\lambda$, set $k=k+1, z_i=k$, and $\boldsymbol{\mu}_k=\boldsymbol{x}_i$.
										 Otherwise, set $z_i=\operatorname{argmin}_c d_{i c}$.

										</li>
										<li>
											Generate clusters $\ell_1, \ldots, \ell_k$ based on $z_1, \ldots, z_k: \ell_j=$ $\left\{\boldsymbol{x}_i \mid z_i=j\right\}$
										</li>
										<li>
											For each cluster $\ell_j$, compute $\boldsymbol{\mu}_j=\frac{1}{\left|\ell_j\right|} \sum_{\boldsymbol{x} \in \ell_j} \boldsymbol{x}$.
										</li>
									</ol>

								</li>
							</div>
							
					</section>

					<section data-transition="fade-in slide-out">

						<h3>
							<font color="#FFFF00">  
								Convergence of Objective function of DP means <br>  
							</font> 
							
						</h3>
							<div class="fragment">
							
								<font color="#FFFF00">  
									Theorem: 
								</font>  DP means algorithm  monotonically decreases the following objective until local convergence:
								$$
								\begin{array}{cc}
								\min _{\left\{\ell_{c}\right\}_{c=1}^{k}} & \sum_{c=1}^{k} \sum_{\boldsymbol{x} \in \ell_{c}}\|\boldsymbol{x}-\boldsymbol{\mu}_{c}\|^{2}+\lambda k \\
								\text { where } & \boldsymbol{\mu}_{c}=\frac{1}{\left|\ell_{c}\right|} \sum_{\boldsymbol{x} \in \ell_{c}} \boldsymbol{x} .
								\end{array}
								$$	
							
							</div>
							<div class="fragment">
							
								<font color="#FFFF00">  
									Proof:
								</font>
									The reassignment step in the algorithm results in a non-increasing objective since the distance between
									 a point and its newly assigned cluster mean never increases; for distances greater than $\lambda$,
									  we can generate a new cluster and pay a penalty of $\lambda$ while still decreasing the objective.
									 Similarly, the mean update step results in a non-increasing objective since the mean is the best representative of a cluster in terms of the squared Euclidean distance.
									  The fact that the algorithm will converge locally follows from the fact that the objective function cannot increase,
									and that there are only a finite number of possible clusterings of the data.
							</div>
					</section>	
				</section>
 

				<section data-background="bg-2.jpeg" data-background-opacity=0.4 style="text-align: left; font-size: 26px;" data-auto-animate
				data-auto-animate-easing="cubic-bezier(0.770, 0.000, 0.175, 1.000)">
				<h3>
					<font color="#FFFF00">  
						Experimental Results <br>  
					</font> 
				</h3>
				<section data-transition="fade-in slide-out">
				<h4>
					Normalized Mutual Information (NMI)
				</h4>
				<div>
					Normalized Mutual Information between class labels $Y$ and cluster labels $C$ is defines as:
					$$NMI(Y,C)=\frac{2\times I(Y,C)}{H(Y)+H(C)}$$
				    where, $H$ is the entropy, $I(Y,C)$ is the mutual information between $Y$ and $C$. 
					Mutual Information tells us the reduction in the entropy of class labels that we get if we know the cluster labels.
					NMI takes value between 0 and 1.
				</div>
				<div style="margin-top: 0.6cm;">
					<ul>
						<li class="fragment">
							NMI is a standard measure for determining the quality of clustering	. Higher the NMI, better the quality of clustering.
						</li>
						<li class="fragment">
							Since it is normalized we can measure and
							compare the NMI between different
							clusterings having different number of
							clusters.
						</li>
					
					</ul>
				</div>

				</section>
				<section data-transition="fade-in slide-out">
					<div>
					To demonstrate advantage of this method over DP Gibbes sampler, a synthetic data set of three Gaussians, shown in the left figure was used.
					The middle figure shows the NMI scores of the clusterings produced by DP means and two Gibbs runs (one that learns  $\alpha$ via a
					gamma prior, and another that uses a validation set to tune $\alpha$.)over the first 5000 iterations.
					Additionally, the number of clusters produced by DP-means as a function of $\lambda$ is plotted in right figure, where we see that there is a large interval of $\lambda$
					values where the algorithm returns three clusters.
				    </div>
					<img width="300" src="data.png">
                    <img width="300" src="tune.png">
					<img width="310" src="nmi.png">
				
					</section>
				    <section data-transition="fade-in slide-out">
					<div class="fragment">
						To demonstrate
						comparable accuracies among the methods- k-means, DP-means, and Gibbs sampling, 8
						common UCI data sets were selected.
						in making comparisons to k-means we fix $k$.
						Given an (approximate)
						number of desired clusters $k$, we first initialize a set
						$T$ with the global mean. We iteratively add to $T$ by finding
						the point in the data set which has the maximum distance to
						$T$ (the distance to $T$ is the smallest distance among points
						in $T$). We repeat this $k$ times and return the value of the
						maximum distance to $T$ in round $k$ as $\lambda$.
						The NMI (normalized
						mutual information) is computed between the ground-truth and the computed clusters, and results are averaged over 10 runs. The
						results are shown in the following table:
					
					</div>
				</section>
				<section data-transition="fade-in slide-out">
					<div >
						Table 1. Average NMI scores on a set of UCI data sets.
						<table>
							<tr>
								<td><b>Dataset</b></td>
								<td><b>DP Means</b></td>
								<td><b>K-means</b></td>
								<td><b>Gibbs</b></td>
							</tr>
							<tr>
								<td>Soybean</td>
								<td><font color="#00FF00">  
									.72
								</font></td>
								<td>.66</td>
								<td>.72</td>
							</tr>
							<tr>
								<td>Wine</td>
								<td>.41</td>
								<td>.43</td>
								<td>.42</td>
							</tr>
							<tr>
								<td>Breast Cancer</td>
								<td><font color="#00FF00">  
									.04
								</font></td>
								<td>.03</td>
								<td>.04</td>
							</tr>
							<tr>
								<td>Car</td>
								<td>.07</td>
								<td>.05</td>
								<td>.15</td>
							</tr>
							<tr>
								<td>Balance Scale</td>
								<td><font color="#00FF00">  
									.17
								</font></td>
								<td>.11</td>
								<td>.14</td>
							</tr>
							<tr>
								<td>Vehicle</td>
								<td><font color="#00FF00">  
									.18
								</font></td>
								<td>.18</td>
								<td>.17</td>
							</tr>
							<tr>
								<td>Iris</td>
								<td>.75</td>
								<td>.76</td>
								<td>.73</td>
							</tr>
							<tr>
								<td>Pima</td>
								<td>.02</td>
								<td>.03</td>
								<td>.02</td>
							</tr>
							<hr>
						</table>
					</div>
				    </section>
				</section>
				<section data-background="bg-2.jpeg" data-background-opacity=0.4 style="text-align: left; font-size: 26px;" data-transition="fade-in slide-out">
                    <h3>
						<font color="#FFFF00">  
						Conclusion <br>  
						</font> 
					</h3>
				    <div>
                    This paper outlines
					connections arising between DP mixture models and hard
					clustering algorithms, and develops scalable algorithms, like k-means for
					hard clustering that is capable of retaining some of the benefits of Bayesian
					nonparametric. This algorithm do not require to specify the number of clusters,
					which is a major advantage over k-means. But, it has some disadvantages also, for example,
					the method involves a hyperparameter and sometimes it is difficult to tune the hyperparameter.
				    </div>
					
				</section>
				
				<section data-background="source.gif" data-background-opacity=0.7 style="font-size: 33px;" >
                    <h3 style="color: rgb(11, 202, 245); text-align: center; margin-bottom: 0.8cm; font-size: 60px;">
                        <strong>Thank you</strong>
                    </h3>
				</section>
				
            </div>
        </div>
        <script src="dist/reveal.js"></script>
        <script src="plugin/notes/notes.js"></script>
        <script src="plugin/markdown/markdown.js"></script>
        <script src="plugin/zoom/zoom.js"></script>
        <script src="plugin/highlight/highlight.js"></script>
        <script src="plugin/math/math.js"></script>
        <script src="plugin/chalkboard/plugin.js"></script>
        <script>
            // More info about initialization & config:
            // - https://revealjs.com/initialization/
            // - https://revealjs.com/config/
            Reveal.initialize({
                hash: true,
                transition: 'concave',
				backgroundTransition: 'slide',
                // controls: false,
				controls: true,

                // Display a presentation progress bar
                progress: true,

                // Push each slide change to the browser history
                history: false,

                // Enable keyboard shortcuts for navigation
                keyboard: true,

				slideNumber: true,

				// Can be used to limit the contexts in which the slide number appears
				// - "all":      Always show the slide number
				// - "print":    Only when printing to PDF
				// - "speaker":  Only in the speaker view
				showSlideNumber: 'all',				

                // Loop the presentation
                loop: false,

                // Number of milliseconds between automatically proceeding to the 
                // next slide, disabled when set to 0
                autoSlide: 0,

                // Enable slide navigation via mouse wheel
                mouseWheel: false,

                // Apply a 3D roll to links on hover
                rollingLinks: true,
                // Learn about plugins: https://revealjs.com/plugins/
                // chalkboard: {
                    // src: "chalkboard/chalkboard.json",
                    // toggleChalkboardButton: false,
                    // toggleNotesButton: false,
                // },
                plugins: [ RevealMarkdown, RevealHighlight, RevealNotes, RevealMath ],
            });
        </script>
    </body>
</html>
